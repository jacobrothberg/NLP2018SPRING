{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np \n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagger=PerceptronTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances: 1396\n",
      "number of testing instances: 634\n"
     ]
    }
   ],
   "source": [
    "with open(\"./training_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    train_data = json.loads(data)\n",
    "    \n",
    "print(\"number of training instances:\", len(train_data))\n",
    "\n",
    "with open(\"./test_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    test_data = json.loads(data)\n",
    "print(\"number of testing instances:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentiment': -0.463,\n",
       "  'snippet': 'downgrade',\n",
       "  'target': '$PCAR',\n",
       "  'tweet': 'downgrades $SON $ARI $GG $FLTX $WMC $MFA $IVR $CMI $PCAR $QLIK $AFOP $UNFI #stocks #investing #tradeideas'},\n",
       " {'sentiment': 0.678,\n",
       "  'snippet': ['looking sexy this morning', 'break on volume'],\n",
       "  'target': '$AMZN',\n",
       "  'tweet': \"$AMZN looking sexy this morning...$600 break on volume and it's gone.\"},\n",
       " {'sentiment': 0.377,\n",
       "  'snippet': 'still long term fan!',\n",
       "  'target': '$SBUX',\n",
       "  'tweet': \"@GerberKawasaki stock hasn't moved much since first few weeks after split but still long term fan! $sbux\"},\n",
       " {'sentiment': 0.129,\n",
       "  'snippet': '$TFM will have a way to go price wise to compete with Kroger. $KR',\n",
       "  'target': '$KR',\n",
       "  'tweet': 'Whole foods $WFM may feel price competition but $TFM will have a way to go price wise to compete with Kroger. $KR https://t.co/XBxJVG94mx'},\n",
       " {'sentiment': 0.395,\n",
       "  'snippet': 'iPhone SE Could Be Doing Better Than Expected',\n",
       "  'target': '$AAPL',\n",
       "  'tweet': \"Apple's iPhone SE Could Be Doing Better Than Expected via @forbes https://t.co/21SWqN43wm $AAPL @Localytics @Fiksu\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentiment': 0.323,\n",
       "  'snippet': 'ooks pretty bullish for now',\n",
       "  'target': '$ATVI',\n",
       "  'tweet': \"$ATVI ooks pretty bullish for now. from a short-term perspective, it's got a good chance of maybe sliding back to 33.70 #stocks #investing\"},\n",
       " {'sentiment': 0.579,\n",
       "  'snippet': ['looks really interesting on drop',\n",
       "   'grabbed some options and stock',\n",
       "   'enough tme before earnings to grow the stock',\n",
       "   'growth all sectors'],\n",
       "  'target': '$CSCO',\n",
       "  'tweet': '$CSCO looks really interesting on drop, grabbed some options and stock, enough tme before earnings to grow the stock, growth all sectors'},\n",
       " {'sentiment': 0.294,\n",
       "  'snippet': 'covered some shorts',\n",
       "  'target': '$TSLA',\n",
       "  'tweet': '$TSLA : covered some shorts @ 246.00 for +9.22pts'},\n",
       " {'sentiment': 0.028,\n",
       "  'snippet': 'triple top forming.',\n",
       "  'target': '$TSLA',\n",
       "  'tweet': 'Watching $TSLA W triple top forming.'},\n",
       " {'sentiment': -0.076,\n",
       "  'snippet': 'Whole Foods shareholders vote down activist initiatives',\n",
       "  'target': '$WFM',\n",
       "  'tweet': 'Whole Foods shareholders vote down activist initiatives: https://t.co/652hqv0zAS $WFM'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NTUSD\n",
    "with open(\"./NTUSD-Fin/NTUSD_Fin_word_v1.0.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    NTUSD = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent_dict = {}\n",
    "for i in range(len(NTUSD)):\n",
    "    word_sent_dict[NTUSD[i][\"token\"]] = NTUSD[i][\"market_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_word= ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '@', '#']\n",
    "def remove_stopwords(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    idx = 0\n",
    "    for sentence in sentence_token:\n",
    "        clean_sentence_token = []\n",
    "        for word in sentence:\n",
    "            #if word not in list(stop_words):\n",
    "            word= ''.join(c for c in word if c not in stop_word)\n",
    "            if word != '':\n",
    "                clean_sentence_token.append(word.lower())\n",
    "        sentence_token[idx] = clean_sentence_token\n",
    "        idx = idx + 1\n",
    "    return sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [item[\"tweet\"].lower() for item in train_data]\n",
    "train_X = remove_stopwords(train_X)\n",
    "train_y = np.array([item[\"sentiment\"] for item in train_data],dtype=np.float)\n",
    "\n",
    "test_X = [item[\"tweet\"].lower() for item in test_data]\n",
    "test_X = remove_stopwords(test_X)\n",
    "test_y = np.array([item[\"sentiment\"] for item in test_data],dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentByDict(sent):\n",
    "    sent = sent.split()\n",
    "    sent_value = []\n",
    "    for s in sent:\n",
    "        try: \n",
    "            sent_value.append(word_sent_dict[s])\n",
    "        except:\n",
    "            pass\n",
    "    if sent_value==[]:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.average(sent_value)\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_score(sentence):\n",
    "    sent_value = []\n",
    "    #print(sentence)\n",
    "    sentence_tagged = np.array(nltk.pos_tag(sentence))\n",
    "    #print(sentence_tagged)\n",
    "    for tagged in sentence_tagged:\n",
    "        wn_tag = get_wordnet_tag(tagged[1])\n",
    "        word = tagged[0]\n",
    "        \n",
    "        nltk_sentiwordnet_score = 0.\n",
    "        #get sentiwordnet score\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV,  wn.VERB):            \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if lemma:\n",
    "                synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "                if synsets:\n",
    "                    swn_synset = swn.senti_synset(synsets[0].name())\n",
    "                    nltk_sentiwordnet_score = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    \n",
    "        #get NTUSD dict score\n",
    "        try: \n",
    "            dict_score = word_sent_dict[word]\n",
    "        except:\n",
    "            dict_score = 0.0\n",
    "        \n",
    "        word_score = np.array([nltk_sentiwordnet_score, dict_score], dtype=float)\n",
    "        sent_value.append(word_score)\n",
    "    #print(sent_value)\n",
    "    return np.average(np.array(sent_value), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_pred = np.array([get_score(sentence) for sentence in train_X])\n",
    "train_nor_pred = (((train_pred-np.min(train_pred,axis=0))/ (np.max(train_pred, axis=0)-np.min(train_pred,axis=0))) - 0.5)*2.\n",
    "\n",
    "test_pred = np.array([get_score(sentence) for sentence in test_X])\n",
    "test_nor_pred = (((test_pred-np.min(test_pred, axis=0))/ (np.max(test_pred, axis=0)-np.min(test_pred, axis=0))) - 0.5)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02378999,  0.56439723],\n",
       "       [-0.11492281,  0.57141767],\n",
       "       [-0.32075472,  0.34361474],\n",
       "       ...,\n",
       "       [-0.20754717,  0.56222534],\n",
       "       [-0.37466307,  0.42168027],\n",
       "       [-0.32075472,  0.47319065]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 0.28521670923266274\n",
      "test mse: 0.2614622213278173\n"
     ]
    }
   ],
   "source": [
    "print(\"train mse:\", mean_squared_error(y_pred=train_nor_pred, y_true=train_y))\n",
    "print(\"test mse:\", mean_squared_error(y_pred=test_nor_pred, y_true=test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load RNN based predictoin\n",
    "with open(\"rnn_pred.txt\", \"r\") as f:\n",
    "    rnn_test_pred = f.read().split(\"\\n\")\n",
    "with open(\"rnn_train_pred.txt\", \"r\") as f:\n",
    "    rnn_train_pred = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_train_pred = np.vstack((train_pred, np.array(rnn_train_pred))).astype(np.float).T\n",
    "combine_test_pred = np.vstack((test_pred, np.array(rnn_test_pred))).astype(np.float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1396, 2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear regression for mapping\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_pred, train_y)\n",
    "test_lr_pred = lr.predict(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': 1, 'normalize': False}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse: 0.11045906941504247\n"
     ]
    }
   ],
   "source": [
    "print(\"test mse:\",mean_squared_error(test_lr_pred, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create feature\n",
    "    * average word embedding \n",
    "    * average NTUSD word sentiment\n",
    "2. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X) + len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output .txt for rnn model prediction\n",
    "with open(\"fin_tweet.txt\", \"w\") as f:\n",
    "    f.write(\"id,text\\n\")\n",
    "    for i, line in enumerate(test_X):\n",
    "        line = line.replace(\"\\n\",\" \")\n",
    "        f.write(str(i))\n",
    "        f.write(\",\")\n",
    "        f.write(line)\n",
    "        if i+1 != len(test_X):\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "with open(\"fin_tweet_train.txt\", \"w\") as f:\n",
    "    f.write(\"id,text\\n\")\n",
    "    for i, line in enumerate(train_X):\n",
    "        line = line.replace(\"\\n\",\" \")\n",
    "        f.write(str(i))\n",
    "        f.write(\",\")\n",
    "        f.write(line)\n",
    "        if i+1 != len(train_X):\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
