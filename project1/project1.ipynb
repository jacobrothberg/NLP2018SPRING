{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np \n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagger=PerceptronTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances: 1396\n",
      "number of testing instances: 634\n"
     ]
    }
   ],
   "source": [
    "with open(\"./training_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    train_data = json.loads(data)\n",
    "    \n",
    "print(\"number of training instances:\", len(train_data))\n",
    "\n",
    "with open(\"./test_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    test_data = json.loads(data)\n",
    "print(\"number of testing instances:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet': 'downgrades $SON $ARI $GG $FLTX $WMC $MFA $IVR $CMI $PCAR $QLIK $AFOP $UNFI #stocks #investing #tradeideas',\n",
       "  'target': '$PCAR',\n",
       "  'snippet': 'downgrade',\n",
       "  'sentiment': -0.463},\n",
       " {'tweet': \"$AMZN looking sexy this morning...$600 break on volume and it's gone.\",\n",
       "  'target': '$AMZN',\n",
       "  'snippet': ['looking sexy this morning', 'break on volume'],\n",
       "  'sentiment': 0.678},\n",
       " {'tweet': \"@GerberKawasaki stock hasn't moved much since first few weeks after split but still long term fan! $sbux\",\n",
       "  'target': '$SBUX',\n",
       "  'snippet': 'still long term fan!',\n",
       "  'sentiment': 0.377},\n",
       " {'tweet': 'Whole foods $WFM may feel price competition but $TFM will have a way to go price wise to compete with Kroger. $KR https://t.co/XBxJVG94mx',\n",
       "  'target': '$KR',\n",
       "  'snippet': '$TFM will have a way to go price wise to compete with Kroger. $KR',\n",
       "  'sentiment': 0.129},\n",
       " {'tweet': \"Apple's iPhone SE Could Be Doing Better Than Expected via @forbes https://t.co/21SWqN43wm $AAPL @Localytics @Fiksu\",\n",
       "  'target': '$AAPL',\n",
       "  'snippet': 'iPhone SE Could Be Doing Better Than Expected',\n",
       "  'sentiment': 0.395}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet': \"$ATVI ooks pretty bullish for now. from a short-term perspective, it's got a good chance of maybe sliding back to 33.70 #stocks #investing\",\n",
       "  'target': '$ATVI',\n",
       "  'snippet': 'ooks pretty bullish for now',\n",
       "  'sentiment': 0.323},\n",
       " {'tweet': '$CSCO looks really interesting on drop, grabbed some options and stock, enough tme before earnings to grow the stock, growth all sectors',\n",
       "  'target': '$CSCO',\n",
       "  'snippet': ['looks really interesting on drop',\n",
       "   'grabbed some options and stock',\n",
       "   'enough tme before earnings to grow the stock',\n",
       "   'growth all sectors'],\n",
       "  'sentiment': 0.579},\n",
       " {'tweet': '$TSLA : covered some shorts @ 246.00 for +9.22pts',\n",
       "  'target': '$TSLA',\n",
       "  'snippet': 'covered some shorts',\n",
       "  'sentiment': 0.294},\n",
       " {'tweet': 'Watching $TSLA W triple top forming.',\n",
       "  'target': '$TSLA',\n",
       "  'snippet': 'triple top forming.',\n",
       "  'sentiment': 0.028},\n",
       " {'tweet': 'Whole Foods shareholders vote down activist initiatives: https://t.co/652hqv0zAS $WFM',\n",
       "  'target': '$WFM',\n",
       "  'snippet': 'Whole Foods shareholders vote down activist initiatives',\n",
       "  'sentiment': -0.076}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NTUSD\n",
    "with open(\"./NTUSD-Fin/NTUSD_Fin_word_v1.0.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    NTUSD = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent_dict = {}\n",
    "for i in range(len(NTUSD)):\n",
    "    word_sent_dict[NTUSD[i][\"token\"]] = NTUSD[i][\"market_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_word= ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '@', '#']\n",
    "def remove_stopwords(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    idx = 0\n",
    "    for sentence in sentence_token:\n",
    "        clean_sentence_token = []\n",
    "        for word in sentence:\n",
    "            #if word not in list(stop_words):\n",
    "            word= ''.join(c for c in word if c not in stop_word)\n",
    "            if word != '':\n",
    "                clean_sentence_token.append(word.lower())\n",
    "        sentence_token[idx] = clean_sentence_token\n",
    "        idx = idx + 1\n",
    "    return sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [item[\"tweet\"].lower() for item in train_data]\n",
    "train_X = remove_stopwords(train_X)\n",
    "train_y = np.array([item[\"sentiment\"] for item in train_data],dtype=np.float)\n",
    "\n",
    "test_X = [item[\"tweet\"].lower() for item in test_data]\n",
    "test_X = remove_stopwords(test_X)\n",
    "test_y = np.array([item[\"sentiment\"] for item in test_data],dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentByDict(sent):\n",
    "    sent = sent.split()\n",
    "    sent_value = []\n",
    "    for s in sent:\n",
    "        try: \n",
    "            sent_value.append(word_sent_dict[s])\n",
    "        except:\n",
    "            pass\n",
    "    if sent_value==[]:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.average(sent_value)\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_score(sentence):\n",
    "    sent_value = []\n",
    "    #print(sentence)\n",
    "    sentence_tagged = np.array(nltk.pos_tag(sentence))\n",
    "    #print(sentence_tagged)\n",
    "    for tagged in sentence_tagged:\n",
    "        wn_tag = get_wordnet_tag(tagged[1])\n",
    "        word = tagged[0]\n",
    "        \n",
    "        nltk_sentiwordnet_score = 0.\n",
    "        #get sentiwordnet score\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV,  wn.VERB):            \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if lemma:\n",
    "                synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "                if synsets:\n",
    "                    swn_synset = swn.senti_synset(synsets[0].name())\n",
    "                    nltk_sentiwordnet_score = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    \n",
    "        #get NTUSD dict score\n",
    "        try: \n",
    "            dict_score = word_sent_dict[word]\n",
    "        except:\n",
    "            dict_score = 0.0\n",
    "        \n",
    "        word_score = np.array([nltk_sentiwordnet_score, dict_score], dtype=float)\n",
    "        sent_value.append(word_score)\n",
    "    #print(sent_value)\n",
    "    return np.average(np.array(sent_value), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_pred = np.array([get_score(sentence) for sentence in train_X])\n",
    "train_nor_pred = (((train_pred-np.min(train_pred,axis=0))/ (np.max(train_pred, axis=0)-np.min(train_pred,axis=0))) - 0.5)*2.\n",
    "\n",
    "test_pred = np.array([get_score(sentence) for sentence in test_X])\n",
    "test_nor_pred = (((test_pred-np.min(test_pred, axis=0))/ (np.max(test_pred, axis=0)-np.min(test_pred, axis=0))) - 0.5)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03225806,  0.17272157],\n",
       "       [-0.03225806,  0.6184941 ],\n",
       "       [-0.03225806,  0.32556491],\n",
       "       ...,\n",
       "       [-0.27047146,  0.35498386],\n",
       "       [ 0.01075269,  0.26635407],\n",
       "       [-0.03225806,  0.35768209]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"train mse:\", mean_squared_error(y_pred=train_nor_pred, y_true=train_y))\n",
    "# print(\"test mse:\", mean_squared_error(y_pred=test_nor_pred, y_true=test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load RNN based predictoin\n",
    "# with open(\"rnn_pred.txt\", \"r\") as f:\n",
    "#     rnn_test_pred = f.read().split(\"\\n\")\n",
    "# with open(\"rnn_train_pred.txt\", \"r\") as f:\n",
    "#     rnn_train_pred = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_train_pred = np.vstack((train_pred, np.array(rnn_train_pred))).astype(np.float).T\n",
    "# combine_test_pred = np.vstack((test_pred, np.array(rnn_test_pred))).astype(np.float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 6 learning rate: 0.1 n_estimator 100 mse: 0.09052934812014757\n"
     ]
    }
   ],
   "source": [
    "# use xgboost\n",
    "max_depth = [3,4,5,6]\n",
    "learning_rate = [1,0.1,0.01]\n",
    "n_estimators = [50,100,200]\n",
    "best_perform = [0,0,0,1]\n",
    "for d in max_depth:\n",
    "    for r in learning_rate:\n",
    "        for e in n_estimators:\n",
    "            xbg_regr = xgb.XGBRegressor(max_depth=d,\n",
    "                                       learning_rate=r,\n",
    "                                       n_estimators=e,\n",
    "                                       n_jobs=-1)\n",
    "            xbg_regr.fit(train_pred,train_y)\n",
    "            xbg_regr_pred = xbg_regr.predict(test_pred)\n",
    "#             print(\"depth\",d,\"learning rate:\",r,\"n_estimator\",e)\n",
    "            mse = mean_squared_error(xbg_regr_pred, test_y)\n",
    "#             print(\"test mse:\",mse)\n",
    "            if mse <= best_perform[-1]:\n",
    "                best_perform[-1]= mse\n",
    "                best_perform[0] = d\n",
    "                best_perform[1] = r\n",
    "                best_perform[2] = e\n",
    "print(\"depth\",best_perform[0],\"learning rate:\",best_perform[1],\n",
    "            \"n_estimator\",best_perform[2], \"mse:\",best_perform[3])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear regression for mapping\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_pred, train_y)\n",
    "test_lr_pred = lr.predict(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test mse:\",mean_squared_error(test_lr_pred, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create feature\n",
    "    * average word embedding \n",
    "    * average NTUSD word sentiment\n",
    "2. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X) + len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output .txt for rnn model prediction\n",
    "with open(\"fin_tweet.txt\", \"w\") as f:\n",
    "    f.write(\"id,text\\n\")\n",
    "    for i, line in enumerate(test_X):\n",
    "        line = line.replace(\"\\n\",\" \")\n",
    "        f.write(str(i))\n",
    "        f.write(\",\")\n",
    "        f.write(line)\n",
    "        if i+1 != len(test_X):\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "with open(\"fin_tweet_train.txt\", \"w\") as f:\n",
    "    f.write(\"id,text\\n\")\n",
    "    for i, line in enumerate(train_X):\n",
    "        line = line.replace(\"\\n\",\" \")\n",
    "        f.write(str(i))\n",
    "        f.write(\",\")\n",
    "        f.write(line)\n",
    "        if i+1 != len(train_X):\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
