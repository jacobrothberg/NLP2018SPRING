{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagger=PerceptronTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances: 1396\n",
      "number of testing instances: 634\n"
     ]
    }
   ],
   "source": [
    "with open(\"./training_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    train_data = json.loads(data)\n",
    "    \n",
    "print(\"number of training instances:\", len(train_data))\n",
    "\n",
    "with open(\"./test_set.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    test_data = json.loads(data)\n",
    "print(\"number of testing instances:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet': 'downgrades $SON $ARI $GG $FLTX $WMC $MFA $IVR $CMI $PCAR $QLIK $AFOP $UNFI #stocks #investing #tradeideas',\n",
       "  'target': '$PCAR',\n",
       "  'snippet': 'downgrade',\n",
       "  'sentiment': -0.463},\n",
       " {'tweet': \"$AMZN looking sexy this morning...$600 break on volume and it's gone.\",\n",
       "  'target': '$AMZN',\n",
       "  'snippet': ['looking sexy this morning', 'break on volume'],\n",
       "  'sentiment': 0.678},\n",
       " {'tweet': \"@GerberKawasaki stock hasn't moved much since first few weeks after split but still long term fan! $sbux\",\n",
       "  'target': '$SBUX',\n",
       "  'snippet': 'still long term fan!',\n",
       "  'sentiment': 0.377},\n",
       " {'tweet': 'Whole foods $WFM may feel price competition but $TFM will have a way to go price wise to compete with Kroger. $KR https://t.co/XBxJVG94mx',\n",
       "  'target': '$KR',\n",
       "  'snippet': '$TFM will have a way to go price wise to compete with Kroger. $KR',\n",
       "  'sentiment': 0.129},\n",
       " {'tweet': \"Apple's iPhone SE Could Be Doing Better Than Expected via @forbes https://t.co/21SWqN43wm $AAPL @Localytics @Fiksu\",\n",
       "  'target': '$AAPL',\n",
       "  'snippet': 'iPhone SE Could Be Doing Better Than Expected',\n",
       "  'sentiment': 0.395}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet': 'Thank you $GOOG (Google Alphabet) and $FB (Facebook) stocks! What a nice reversal.',\n",
       "  'target': '$GOOG',\n",
       "  'snippet': 'What a nice reversal.',\n",
       "  'sentiment': 0.318},\n",
       " {'tweet': 'Short $TSLA calls',\n",
       "  'target': '$TSLA',\n",
       "  'snippet': 'Short $TSLA calls',\n",
       "  'sentiment': -0.618},\n",
       " {'tweet': '$CTRP https://t.co/MvviVQUIx0 $46 Breakout Should be Confirmed with Wm%R Stochastic Upturn https://t.co/tnU9yRsbNU https://t.co/pS7y56bhET',\n",
       "  'target': '$CTRP',\n",
       "  'snippet': ['Breakout Should be Confirmed', 'Upturn'],\n",
       "  'sentiment': 0.448},\n",
       " {'tweet': 'Dow worst performers Chevron -0.47 and Nike -0.52 Best performers IBM +1.33 and Apple +1.48 $AAPL $NKE $IBM #stocks #investing',\n",
       "  'target': '$AAPL',\n",
       "  'snippet': 'Best performers',\n",
       "  'sentiment': 0.376},\n",
       " {'tweet': '$TSLA borrow still -7.5% which means very tight on the avail stock for shorting....  $VALE meanwhile easing up to \"only -8%',\n",
       "  'target': '$VALE',\n",
       "  'snippet': 'very tight on the avail stock for shorting',\n",
       "  'sentiment': -0.406},\n",
       " {'tweet': '@cek_cpa @ryanwallace198 @financialbuzz Guys - get into $WFM and $MJN.',\n",
       "  'target': '$WFM',\n",
       "  'snippet': 'get into $WFM and $MJN',\n",
       "  'sentiment': 0.415}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[9:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./SCL-NMA.txt\", \"r\") as f:\n",
    "    MaxDiff = f.readlines()\n",
    "    MaxDiff_dict = {}\n",
    "    for line in MaxDiff:\n",
    "        MaxDiff_dict[line.strip().split(\"\\t\")[0]] = line.strip().split(\"\\t\")[1]\n",
    "\n",
    "afinn = Afinn()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# load NTUSD\n",
    "with open(\"./NTUSD-Fin/NTUSD_Fin_word_v1.0.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    NTUSD = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent_dict = {}\n",
    "for i in range(len(NTUSD)):\n",
    "    word_sent_dict[NTUSD[i][\"token\"]] = NTUSD[i][\"market_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_word= ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '@', '#']\n",
    "def remove_stopwords(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    idx = 0\n",
    "    for sentence in sentence_token:\n",
    "        clean_sentence_token = []\n",
    "        for word in sentence:\n",
    "            #if word not in list(stop_words):\n",
    "            word= ''.join(c for c in word if c not in stop_word)\n",
    "            if word != '':\n",
    "                clean_sentence_token.append(word.lower())\n",
    "        sentence_token[idx] = clean_sentence_token\n",
    "        idx = idx + 1\n",
    "    return sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [item[\"tweet\"].lower() for item in train_data]\n",
    "train_X = remove_stopwords(train_X)\n",
    "train_y = np.array([item[\"sentiment\"] for item in train_data],dtype=np.float)\n",
    "\n",
    "test_X = [item[\"tweet\"].lower() for item in test_data]\n",
    "test_X = remove_stopwords(test_X)\n",
    "test_y = np.array([item[\"sentiment\"] for item in test_data],dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentByDict(sent):\n",
    "    sent = sent.split()\n",
    "    sent_value = []\n",
    "    for s in sent:\n",
    "        try: \n",
    "            sent_value.append(word_sent_dict[s])\n",
    "        except:\n",
    "            pass\n",
    "    if sent_value==[]:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.average(sent_value)\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_score(sentence):\n",
    "    sent_value = []\n",
    "    #print(sentence)\n",
    "    sentence_tagged = np.array(nltk.pos_tag(sentence))\n",
    "    #print(sentence_tagged)\n",
    "    for tagged in sentence_tagged:\n",
    "        wn_tag = get_wordnet_tag(tagged[1])\n",
    "        word = tagged[0]\n",
    "        \n",
    "        sentiwordnet_neg = 0.\n",
    "        sentiwordnet_pos = 0.\n",
    "        #get sentiwordnet score\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV,  wn.VERB):            \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if lemma:\n",
    "                synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "                if synsets:\n",
    "                    swn_synset = swn.senti_synset(synsets[0].name())\n",
    "                    sentiwordnet_neg = swn_synset.neg_score()\n",
    "                    sentiwordnet_pos = swn_synset.pos_score()\n",
    "    \n",
    "        #get NTUSD dict score\n",
    "        try: \n",
    "            dict_score = word_sent_dict[word]\n",
    "        except:\n",
    "            dict_score = 0.0\n",
    "            \n",
    "        s = \" \".join(sentence)\n",
    "        # affin\n",
    "        afinn_score = afinn.score(s)\n",
    "        \n",
    "        # vader\n",
    "        vader_output = analyzer.polarity_scores(s)\n",
    "        vader_neg = vader_output[\"neg\"]\n",
    "        vader_pos = vader_output[\"pos\"]\n",
    "        vader_neu = vader_output[\"neu\"]\n",
    "        \n",
    "        word_score = np.array([sentiwordnet_pos,sentiwordnet_neg , dict_score,afinn_score, vader_neg, vader_pos], dtype=float)\n",
    "        sent_value.append(word_score)\n",
    "    #print(sent_value)\n",
    "    return np.average(np.array(sent_value), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_pred = np.array([get_score(sentence) for sentence in train_X])\n",
    "train_nor_pred = (((train_pred-np.min(train_pred,axis=0))/ (np.max(train_pred, axis=0)-np.min(train_pred,axis=0))) - 0.5)*2.\n",
    "\n",
    "test_pred = np.array([get_score(sentence) for sentence in test_X])\n",
    "test_nor_pred = (((test_pred-np.min(test_pred, axis=0))/ (np.max(test_pred, axis=0)-np.min(test_pred, axis=0))) - 0.5)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load RNN based predictoin\n",
    "# with open(\"rnn_pred.txt\", \"r\") as f:\n",
    "#     rnn_test_pred = f.read().split(\"\\n\")\n",
    "# with open(\"rnn_train_pred.txt\", \"r\") as f:\n",
    "#     rnn_train_pred = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_train_pred = np.vstack((train_pred, np.array(rnn_train_pred))).astype(np.float).T\n",
    "# combine_test_pred = np.vstack((test_pred, np.array(rnn_test_pred))).astype(np.float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 7 learning rate: 0.1 n_estimator 200 mse: 0.0832770475630163\n"
     ]
    }
   ],
   "source": [
    "# use xgboost\n",
    "max_depth = [7]\n",
    "learning_rate = [0.1]\n",
    "n_estimators = [200]\n",
    "best_perform = [0,0,0,1]\n",
    "for d in max_depth:\n",
    "    for r in learning_rate:\n",
    "        for e in n_estimators:\n",
    "            xbg_regr = xgb.XGBRegressor(max_depth=d,\n",
    "                                       learning_rate=r,\n",
    "                                       n_estimators=e,\n",
    "                                       n_jobs=-1)\n",
    "            xbg_regr.fit(train_pred,train_y)\n",
    "            xbg_regr_pred = xbg_regr.predict(test_pred)\n",
    "#             print(\"depth\",d,\"learning rate:\",r,\"n_estimator\",e)\n",
    "            mse = mean_squared_error(xbg_regr_pred, test_y)\n",
    "#             print(\"test mse:\",mse)\n",
    "            if mse <= best_perform[-1]:\n",
    "                best_perform[-1]= mse\n",
    "                best_perform[0] = d\n",
    "                best_perform[1] = r\n",
    "                best_perform[2] = e\n",
    "                \n",
    "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xbg_regr,f)\n",
    "    \n",
    "print(\"depth:\",best_perform[0],\"learning rate:\",best_perform[1],\n",
    "            \"n_estimator\",best_perform[2], \"mse:\",best_perform[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear regression for mapping\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_pred, train_y)\n",
    "test_lr_pred = lr.predict(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': 1, 'normalize': False}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse: 0.10446757795527752\n"
     ]
    }
   ],
   "source": [
    "print(\"test mse:\",mean_squared_error(test_lr_pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output .txt for rnn model prediction\n",
    "# with open(\"fin_tweet.txt\", \"w\") as f:\n",
    "#     f.write(\"id,text\\n\")\n",
    "#     for i, line in enumerate(test_X):\n",
    "#         line = line.replace(\"\\n\",\" \")\n",
    "#         f.write(str(i))\n",
    "#         f.write(\",\")\n",
    "#         f.write(line)\n",
    "#         if i+1 != len(test_X):\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "# with open(\"fin_tweet_train.txt\", \"w\") as f:\n",
    "#     f.write(\"id,text\\n\")\n",
    "#     for i, line in enumerate(train_X):\n",
    "#         line = line.replace(\"\\n\",\" \")\n",
    "#         f.write(str(i))\n",
    "#         f.write(\",\")\n",
    "#         f.write(line)\n",
    "#         if i+1 != len(train_X):\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
