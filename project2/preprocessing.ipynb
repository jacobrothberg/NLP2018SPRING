{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print (lemmatizer.lemmatize('working', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(FILE_Path):\n",
    "    otuput_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(FILE_Path, \"r\") as f:\n",
    "        train_tmp = f.read().split(\"\\n\\n\")[:-1]\n",
    "        for sample in train_tmp:\n",
    "            sentence = re.split('\\t|\\n', sample)[1].replace('\\\"',\"\").replace(\".\",\"\").replace(\",\",\"\")\\\n",
    "            .replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"\\'s\",\"\")\\\n",
    "            .replace(\"\\'ve\",\"\").replace(\"\\'t\",\"\").replace(\"\\'re\",\"\").replace(\"\\'d\",\"\")\\\n",
    "            .replace(\"\\'ll\",\"\").replace(\"'\",\"\").replace(\";\",\"\").replace(\":\",\"\")\n",
    "            answer = re.split('\\t|\\n', sample)[2]\n",
    "            soup = BeautifulSoup(sentence,\"lxml\")\n",
    "\n",
    "            e1 = str(soup.find('e1'))[4:-5]\n",
    "            e2 = str(soup.find('e2'))[4:-5]\n",
    "            word_list = sentence.split(\" \")\n",
    "            e1_check = 0\n",
    "            e2_check = 0\n",
    "            for word in word_list:\n",
    "                if word.endswith(\"</e1>\"):\n",
    "                    e1_check = 1\n",
    "                if word.endswith(\"</e2>\"):\n",
    "                    e2_check = 1\n",
    "            if e1_check + e2_check != 2:\n",
    "                print(sentence)\n",
    "            for word in word_list:\n",
    "                if \"</e1>\" in word:\n",
    "                    a = word\n",
    "                if \"</e2>\" in word:\n",
    "                    b = word\n",
    "            p1 = word_list.index(a)\n",
    "            p2 = word_list.index(b)\n",
    "            sentence = sentence.replace(\"<e1>\",\"\").replace(\"</e1>\",\"\").replace(\"<e2>\",\"\").\\\n",
    "            replace(\"</e2>\",\"\")\n",
    "            sentence = [lemmatizer.lemmatize(word, pos='v') for word in sentence.split(\" \")]\n",
    "            sentence = \" \".join(sentence)\n",
    "            otuput_sent.append((sentence,e1,e2,p1,p2,answer))\n",
    "    return otuput_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n"
     ]
    }
   ],
   "source": [
    "train_instance = load_data(\"./data/TRAIN_FILE.txt\")\n",
    "print(\"--------\")\n",
    "test_instance = load_data(\"./data/TEST_FILE_FULL.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When thecoil of the relay be at rest not energize the common terminal 30 and the normally close terminal 87a have continuity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_text[4783])\n",
    "train_position[4783]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_position = [(ele[3],ele[4]) for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check p1 p2 order\n",
    "for a, b in train_position:\n",
    "    if a > b: \n",
    "        print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]\n",
    "test_test = [ele[0] for ele in test_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great wing of fluorite dragons the greatest concentration of the rather solitary dragons ever known joined the desperate battle with the blues and silvers against Chaos\n",
      "wing NN nsubj xxxx False 0 [concentration, joined] lefts ['A', 'great'] rights ['of'] 2 1\n",
      "dragons NNS pobj xxxx False 0 [of, wing, concentration, joined] lefts ['fluorite'] rights [] 1 0\n"
     ]
    }
   ],
   "source": [
    "demo_id = 62\n",
    "demo_sentence = train_text[demo_id]\n",
    "print(demo_sentence)\n",
    "\n",
    "doc = nlp(demo_sentence)\n",
    "\n",
    "for pos, token in enumerate(doc):\n",
    "    if pos in train_position[demo_id]:\n",
    "        ancestors = [i for i in token.ancestors]\n",
    "        print(token.text, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_stop, token.ent_type,ancestors,\n",
    "              \"lefts\",[word.text for word in token.lefts],\n",
    "              \"rights\",[word.text for word in token.rights],\n",
    "              token.n_lefts,  # 2\n",
    "            token.n_rights  # 1\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = []\n",
    "train_dep_list = []\n",
    "for ele in train_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    train_mut_ancestors_list.append(mut_ancestors)\n",
    "    train_dep_list.append(both_dep)\n",
    "    \n",
    "test_mut_ancestors_list = []\n",
    "test_dep_list = []\n",
    "for ele in test_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    test_mut_ancestors_list.append(mut_ancestors)\n",
    "    test_dep_list.append(both_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./features/train_mut_ancestors_list.npy\", train_mut_ancestors_list)\n",
    "np.save(\"./features/test_mut_ancestors_list.npy\", test_mut_ancestors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b = np.unique(train_dep_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_sorted = a[np.argsort(b)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nsubjpobj', 'dobjpobj', 'pobjpobj', 'nsubjdobj', 'nsubjpasspobj',\n",
       "       'compoundpobj', 'nsubjcompound', 'attrpobj', 'compoundnsubj',\n",
       "       'compounddobj', 'conjpobj', 'pobjdobj', 'nsubjnsubj', 'nsubjpunct',\n",
       "       'pobjcompound', 'compoundcompound', 'dobjdobj', 'amodpobj',\n",
       "       'dobjcompound', 'ROOTpobj', 'nsubjconj', 'nsubjpasscompound',\n",
       "       'nsubjamod', 'nsubjpassdobj', 'attrdobj', 'punctprep', 'appospobj',\n",
       "       'pobjconj', 'pobjnsubj', 'compoundattr', 'nsubjnmod', 'pobjattr',\n",
       "       'amoddobj'], dtype='<U18')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_dep = a_sorted[:33]\n",
    "major_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1784,\n",
       " 928,\n",
       " 838,\n",
       " 833,\n",
       " 688,\n",
       " 320,\n",
       " 172,\n",
       " 151,\n",
       " 131,\n",
       " 111,\n",
       " 110,\n",
       " 87,\n",
       " 83,\n",
       " 65,\n",
       " 64,\n",
       " 54,\n",
       " 50,\n",
       " 50,\n",
       " 46,\n",
       " 46,\n",
       " 45,\n",
       " 40,\n",
       " 39,\n",
       " 38,\n",
       " 27,\n",
       " 27,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 22,\n",
       " 20,\n",
       " 20]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b,reverse=True)[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dep_list_filter = []\n",
    "for dep in train_dep_list:\n",
    "    if dep in major_dep:\n",
    "        train_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        train_dep_list_filter.append(\"other\")\n",
    "\n",
    "test_dep_list_filter = []\n",
    "for dep in test_dep_list:\n",
    "    if dep in major_dep:\n",
    "        test_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        test_dep_list_filter.append(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ROOTpobj', 'amoddobj', 'amodpobj', 'appospobj', 'attrdobj',\n",
       "        'attrpobj', 'compoundattr', 'compoundcompound', 'compounddobj',\n",
       "        'compoundnsubj', 'compoundpobj', 'conjpobj', 'dobjcompound',\n",
       "        'dobjdobj', 'dobjpobj', 'nsubjamod', 'nsubjcompound', 'nsubjconj',\n",
       "        'nsubjdobj', 'nsubjnmod', 'nsubjnsubj', 'nsubjpasscompound',\n",
       "        'nsubjpassdobj', 'nsubjpasspobj', 'nsubjpobj', 'nsubjpunct',\n",
       "        'other', 'pobjattr', 'pobjcompound', 'pobjconj', 'pobjdobj',\n",
       "        'pobjnsubj', 'pobjpobj', 'punctprep'], dtype='<U17'),\n",
       " array([ 22,   7,  13,   3,  11,  61,   8,  16,  38,  25, 102,  25,  16,\n",
       "         22, 311,  12,  45,  22, 246,   8,  25,  22,  15, 239, 648,  27,\n",
       "        340,  10,  28,   8,  38,   9, 283,  12]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_dep_list_filter, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform string to onehot\n",
    "train_dep_list_filter = lb.fit_transform(train_dep_list_filter)\n",
    "test_dep_list_filter = lb.transform(test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./features/train_dep_list.npy\", train_dep_list_filter)\n",
    "np.save(\"./features/test_dep_list.npy\", test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dep_list_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dep_list_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
