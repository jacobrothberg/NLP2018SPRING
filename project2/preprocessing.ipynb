{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pickle as pk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print (lemmatizer.lemmatize('working', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(FILE_Path):\n",
    "    otuput_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(FILE_Path, \"r\") as f:\n",
    "        train_tmp = f.read().split(\"\\n\\n\")[:-1]\n",
    "        for sample in train_tmp:\n",
    "            sentence = re.split('\\t|\\n', sample)[1].replace('\\\"',\"\").replace(\".\",\"\").replace(\",\",\"\")\\\n",
    "            .replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"\\'s\",\"\")\\\n",
    "            .replace(\"\\'ve\",\"\").replace(\"\\'t\",\"\").replace(\"\\'re\",\"\").replace(\"\\'d\",\"\")\\\n",
    "            .replace(\"\\'ll\",\"\").replace(\"'\",\"\").replace(\";\",\"\").replace(\":\",\"\")\n",
    "            answer = re.split('\\t|\\n', sample)[2]\n",
    "            soup = BeautifulSoup(sentence,\"lxml\")\n",
    "\n",
    "            e1 = str(soup.find('e1'))[4:-5]\n",
    "            e2 = str(soup.find('e2'))[4:-5]\n",
    "            word_list = sentence.split(\" \")\n",
    "            e1_check = 0\n",
    "            e2_check = 0\n",
    "            for word in word_list:\n",
    "                if word.endswith(\"</e1>\"):\n",
    "                    e1_check = 1\n",
    "                if word.endswith(\"</e2>\"):\n",
    "                    e2_check = 1\n",
    "            if e1_check + e2_check != 2:\n",
    "                print(sentence)\n",
    "            for word in word_list:\n",
    "                if \"</e1>\" in word:\n",
    "                    a = word\n",
    "                if \"</e2>\" in word:\n",
    "                    b = word\n",
    "            p1 = word_list.index(a)\n",
    "            p2 = word_list.index(b)\n",
    "            sentence = sentence.replace(\"<e1>\",\"\").replace(\"</e1>\",\"\").replace(\"<e2>\",\"\").\\\n",
    "            replace(\"</e2>\",\"\")\n",
    "            sentence = [lemmatizer.lemmatize(word, pos='v') for word in sentence.split(\" \")]\n",
    "            sentence = \" \".join(sentence)\n",
    "            otuput_sent.append((sentence,e1,e2,p1,p2,answer))\n",
    "    return otuput_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n"
     ]
    }
   ],
   "source": [
    "train_instance = load_data(\"./data/TRAIN_FILE.txt\")\n",
    "print(\"--------\")\n",
    "test_instance = load_data(\"./data/TEST_FILE_FULL.txt\")\n",
    "total = train_instance + test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When thecoil of the relay be at rest not energize the common terminal 30 and the normally close terminal 87a have continuity\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_position' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fd9bd3a1b8bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4783\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_position\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4783\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_position' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_text[4783])\n",
    "train_position[4783]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_position = [(ele[3],ele[4]) for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check p1 p2 order\n",
    "for a, b in train_position:\n",
    "    if a > b: \n",
    "        print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]\n",
    "test_test = [ele[0] for ele in test_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_id = 62\n",
    "demo_sentence = train_text[demo_id]\n",
    "print(demo_sentence)\n",
    "\n",
    "doc = nlp(demo_sentence)\n",
    "\n",
    "for pos, token in enumerate(doc):\n",
    "    if pos in train_position[demo_id]:\n",
    "        ancestors = [i for i in token.ancestors]\n",
    "        print(token.text, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_stop, token.ent_type,ancestors,\n",
    "              \"lefts\",[word.text for word in token.lefts],\n",
    "              \"rights\",[word.text for word in token.rights],\n",
    "              token.n_lefts,  # 2\n",
    "            token.n_rights  # 1\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = []\n",
    "train_dep_list = []\n",
    "for ele in train_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    train_mut_ancestors_list.append(mut_ancestors)\n",
    "    train_dep_list.append(both_dep)\n",
    "    \n",
    "test_mut_ancestors_list = []\n",
    "test_dep_list = []\n",
    "for ele in test_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    test_mut_ancestors_list.append(mut_ancestors)\n",
    "    test_dep_list.append(both_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_mut_ancestors_list.npy\", train_mut_ancestors_list)\n",
    "np.save(\"./features/test_mut_ancestors_list.npy\", test_mut_ancestors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(train_dep_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sorted = a[np.argsort(b)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_dep = a_sorted[:33]\n",
    "major_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-86907b73beb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "sorted(b,reverse=True)[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dep_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0cee17ed6a6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_dep_list_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dep_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmajor_dep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtrain_dep_list_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dep_list' is not defined"
     ]
    }
   ],
   "source": [
    "train_dep_list_filter = []\n",
    "for dep in train_dep_list:\n",
    "    if dep in major_dep:\n",
    "        train_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        train_dep_list_filter.append(\"other\")\n",
    "\n",
    "test_dep_list_filter = []\n",
    "for dep in test_dep_list:\n",
    "    if dep in major_dep:\n",
    "        test_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        test_dep_list_filter.append(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dep_list_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0212834e3006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dep_list_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dep_list_filter' is not defined"
     ]
    }
   ],
   "source": [
    "np.unique(test_dep_list_filter, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform string to onehot\n",
    "train_dep_list_filter = lb.fit_transform(train_dep_list_filter)\n",
    "test_dep_list_filter = lb.transform(test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./features/train_dep_list.npy\", train_dep_list_filter)\n",
    "np.save(\"./features/test_dep_list.npy\", test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dep_list_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dep_list_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## WordNet hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_list = []\n",
    "for idx in range(len(total)):\n",
    "    entity_1 = total[idx][1]\n",
    "    entity_2 = total[idx][2]\n",
    "    \n",
    "    synsets = wn.synsets(entity_1, pos=wn.NOUN)\n",
    "    hyper_list_1 = []\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            hyper_list_1 = hyper_list_1 + [x.lower() for x in hyper.lemma_names() if '_' not in x]\n",
    "    \n",
    "    synsets = wn.synsets(entity_2, pos=wn.NOUN)\n",
    "    hyper_list_2 = []\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            hyper_list_2 = hyper_list_2 + [x.lower() for x in hyper.lemma_names() if '_' not in x]\n",
    "    hyper = ' '.join(list(set(hyper_list_1).union(set(hyper_list_2)))) \n",
    "    hyper_list.append(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hyper_list = hyper_list[:8000]\n",
    "test_hyper_list = hyper_list[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_hyper_list.npy\", train_hyper_list)\n",
    "np.save(\"./features/test_hyper_list.npy\", test_hyper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rootage descendant descendent somebody individual offspring mortal issue someone root soul trough juvenile progeny beginning birth person source origin'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hyper_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PropBank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict/propBank_dict.pk', 'rb') as f:\n",
    "    propBank_dict = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "propBank_verb_entity_dict = {}\n",
    "hyper_verb_list = []\n",
    "for idx in range(len(total)):\n",
    "    prop_rule = []\n",
    "    temp_dict = {}\n",
    "    for prop_rule in propBank_dict[idx]['propBank']:\n",
    "        verb = prop_rule[0].split('.')[0]\n",
    "        verb = lmtzr.lemmatize(verb,'v')\n",
    "        entity = prop_rule[4]\n",
    "        if len(entity.split(' ')) > 1:\n",
    "            entity = entity.split(' ')[-1]\n",
    "        #print(entity)\n",
    "        if verb in temp_dict:\n",
    "            temp_dict[verb].append(entity)\n",
    "        else:\n",
    "            temp_dict[verb] = [entity]\n",
    "    \n",
    "    temp_hyper_list = []\n",
    "    for k, v in temp_dict.items():\n",
    "        if total[idx][1] in v and total[idx][2] in v:\n",
    "            synsets = wn.synsets(k, pos=wn.VERB)\n",
    "            temp_hyper_list.append(k)\n",
    "\n",
    "            for syn in synsets:\n",
    "                if len(syn.hypernyms()) != 0:\n",
    "                    temp_hyper_list = temp_hyper_list + [x.lower() for x in syn.hypernyms()[0].lemma_names() if '_' not in x]\n",
    "            \n",
    "    hyper = ' '.join(list(set(temp_hyper_list)))\n",
    "    hyper_verb_list.append(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'relate fix fasten stick adhere indispose restrain bond bind attach secure cover hold confine',\n",
       " 'ingest use work have take act consume move exploit',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_verb_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hyper_verb_list = hyper_verb_list[:8000]\n",
    "test_hyper_verb_list = hyper_verb_list[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_hyper_verb_list.npy\", train_hyper_verb_list)\n",
    "np.save(\"./features/test_hyper_verb_list.npy\", test_hyper_verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
