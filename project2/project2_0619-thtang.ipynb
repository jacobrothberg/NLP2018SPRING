{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle as pk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding,Input,BatchNormalization,Dense,Bidirectional,LSTM,Dropout\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.merge import add, dot, concatenate, multiply, average\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LabelsMapping = {'Other': 0,\n",
    "                 'Message-Topic(e1,e2)': 1, 'Message-Topic(e2,e1)': 2,\n",
    "                 'Product-Producer(e1,e2)': 3, 'Product-Producer(e2,e1)': 4,\n",
    "                 'Instrument-Agency(e1,e2)': 5, 'Instrument-Agency(e2,e1)': 6,\n",
    "                 'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
    "                 'Cause-Effect(e1,e2)': 9, 'Cause-Effect(e2,e1)': 10,\n",
    "                 'Component-Whole(e1,e2)': 11, 'Component-Whole(e2,e1)': 12,\n",
    "                 'Entity-Origin(e1,e2)': 13, 'Entity-Origin(e2,e1)': 14,\n",
    "                 'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16,\n",
    "                 'Content-Container(e1,e2)': 17, 'Content-Container(e2,e1)': 18}\n",
    "\n",
    "def _shuffle(X, feature1, feature2, y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], feature1[randomize], feature2[randomize], y[randomize])\n",
    "\n",
    "def load_data(FILE_Path):\n",
    "    otuput_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(FILE_Path, \"r\") as f:\n",
    "        train_tmp = f.read().split(\"\\n\\n\")[:-1]\n",
    "        for sample in train_tmp:\n",
    "            sentence = re.split('\\t|\\n', sample)[1].replace('\\\"',\"\").replace(\".\",\"\").replace(\",\",\"\")\\\n",
    "            .replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"\\'s\",\"\")\\\n",
    "            .replace(\"\\'ve\",\"\").replace(\"\\'t\",\"\").replace(\"\\'re\",\"\").replace(\"\\'d\",\"\").replace(\"\\'ll\",\"\")\n",
    "            answer = re.split('\\t|\\n', sample)[2]\n",
    "            soup = BeautifulSoup(sentence,\"lxml\")\n",
    "\n",
    "            e1 = str(soup.find('e1'))[4:-5]\n",
    "            e2 = str(soup.find('e2'))[4:-5]\n",
    "            word_list = sentence.split(\" \")\n",
    "            for word in word_list:\n",
    "                if \"</e1>\" in word:\n",
    "                    a = word\n",
    "                if \"</e2>\" in word:\n",
    "                    b = word\n",
    "            p1 = word_list.index(a)\n",
    "            p2 = word_list.index(b)\n",
    "            sentence = sentence.replace(\"<e1>\",\"\").replace(\"</e1>\",\"\").replace(\"<e2>\",\"\").\\\n",
    "            replace(\"</e2>\",\"\")\n",
    "#             sentence = [lemmatizer.lemmatize(word, pos='v') for word in sentence.split(\" \")]\n",
    "#             sentence = \" \".join(sentence)\n",
    "            otuput_sent.append((sentence,e1,e2,p1,p2,answer))\n",
    "    return otuput_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances: 8000\n",
      "number of testing instances: 2717\n"
     ]
    }
   ],
   "source": [
    "train_instance = load_data('./data/TRAIN_FILE.txt')\n",
    "test_instance = load_data('./data/TEST_FILE_FULL.txt')\n",
    "# sentence_list = train_sentence + test_sentence\n",
    "print(\"number of training instances:\", len(train_instance))\n",
    "print(\"number of testing instances:\", len(test_instance))\n",
    "# tuple format: (text, e1, e2, pos1, pos2, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "num_classes = 19\n",
    "train_text = []\n",
    "for instance in train_instance:\n",
    "    p1 = instance[3]\n",
    "    p2 = instance[4]\n",
    "    prune_text = \" \".join(instance[0].split(\" \")[p1:p2+1])\n",
    "    train_text.append(prune_text)\n",
    "\n",
    "train_label = [LabelsMapping[ele[5]] for ele in train_instance]\n",
    "train_label = dense_to_one_hot(np.array(train_label), 19)\n",
    "\n",
    "test_text = []\n",
    "for instance in test_instance:\n",
    "    p1 = instance[3]\n",
    "    p2 = instance[4]\n",
    "    prune_text = \" \".join(instance[0].split(\" \")[p1:p2+1])\n",
    "    test_text.append(prune_text)\n",
    "    \n",
    "test_label = [LabelsMapping[ele[5]] for ele in test_instance]\n",
    "test_label = dense_to_one_hot(np.array(test_label), 19)\n",
    "\n",
    "total_text = train_text + test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = np.load(\"./features/train_mut_ancestors_list.npy\")\n",
    "test_mut_ancestors_list = np.load(\"./features/test_mut_ancestors_list.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dep_list = np.load(\"./features/train_dep_list.npy\")\n",
    "test_dep_list = np.load(\"./features/test_dep_list.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11518 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=25000,lower=True,split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(total_text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 33\n"
     ]
    }
   ],
   "source": [
    "train_sentence_seq = tokenizer.texts_to_sequences(train_text)\n",
    "test_sentence_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "max_length = np.max([len(i) for i in train_sentence_seq+test_sentence_seq])\n",
    "print(\"max length:\", max_length)\n",
    "\n",
    "x_train_seq = sequence.pad_sequences(train_sentence_seq, maxlen=max_length)\n",
    "x_test_seq = sequence.pad_sequences(test_sentence_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# download pre-trained word vector from \"https://nlp.stanford.edu/projects/glove/\"\n",
    "tmp_file = get_tmpfile(\"/home/thtang/LifeLog/data/glove_pretrained/gensim_crawl_300d.txt\")\n",
    "\n",
    "w2vModel = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "embedding_size = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "oov = 0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2vModel.wv.vocab:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = w2vModel[word]\n",
    "    else:\n",
    "        oov+=1\n",
    "print(\"OOV:\",oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Embedding, Input,InputLayer,BatchNormalization, Dense, Bidirectional,LSTM,Dropout,GRU,Activation\n",
    "from keras import backend as K\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "def train_BiLSTM(x_train, ancestor_train, dep_train, y_train, \n",
    "                 x_val, ancestor_val, dep_val, y_val,\n",
    "                 embedding_matrix, max_length, max_features):\n",
    "    embedding_size = 300\n",
    "    batch_size = 64\n",
    "    epochs = 50\n",
    "    embedding_layer = Embedding(max_features,output_dim= embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm0 = Bidirectional(LSTM(256,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(embedded_sequences)\n",
    "    lstm1 = Bidirectional(LSTM(128,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(lstm0)\n",
    "    lstm2 = Bidirectional(LSTM(64,activation=\"tanh\",dropout=0.2,return_sequences = False,\n",
    "                kernel_initializer='he_uniform'))(lstm1)\n",
    "    bn1 = BatchNormalization()(lstm2)\n",
    "    \n",
    "    # other feature inputs \n",
    "    ancestor_input = Input(shape=(2,))\n",
    "    ancestor_feature = Dense(64, activation=swish)(ancestor_input)\n",
    "    \n",
    "    \n",
    "    dep_input = Input(shape=(34,))\n",
    "    dep_feature = Dense(128, activation=swish)(dep_input)\n",
    "    \n",
    "    combine_feature = concatenate([bn1, ancestor_feature, dep_feature])\n",
    "    dense1 = Dense(64, activation=swish)(combine_feature)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(32, activation=swish)(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense2)\n",
    "    preds = Dense(19, activation='softmax')(dropout2)\n",
    "    model = Model([sequence_input, ancestor_input, dep_input], preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    filepath = \"models/BiLSTM_3.hdf5\" \n",
    "    checkpoint = ModelCheckpoint(filepath,monitor='val_acc',save_best_only=True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    history = model.fit([x_train, ancestor_train, dep_train], y_train, \n",
    "                        validation_data=([x_val, ancestor_val, dep_val], y_val), \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X, train_ancestor, train_dep, train_y = _shuffle(x_train_seq, \n",
    "                                                       train_mut_ancestors_list,\n",
    "                                                       train_dep_list,\n",
    "                                                       train_label)\n",
    "model = train_BiLSTM(train_X, train_ancestor, train_dep, train_y, \n",
    "                     x_test_seq, test_mut_ancestors_list, test_dep_list, test_label,\n",
    "                     embedding_matrix,\n",
    "                     max_length,\n",
    "                     len(word_index)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [np.where(r==1)[0][0] for r in test_label ]\n",
    "\n",
    "# model_name = ['models/BiLSTM_0817.hdf5','models/BiLSTM_08175.hdf5',\n",
    "#               'models/BiLSTM_0820.hdf5','models/BiLSTM_0821.hdf5']\n",
    "\n",
    "# model1 = load_model(model_name[0])\n",
    "# prediction1 = model1.predict([x_test_seq], batch_size=1000)\n",
    "# model2 = load_model(model_name[1])\n",
    "# prediction2 = model2.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "# model3 = load_model(model_name[2])\n",
    "# prediction3 = model3.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "# model4 = load_model(model_name[3])\n",
    "# prediction4 = model4.predict([x_test_seq,test_mut_ancestors_list], batch_size=1000)\n",
    "model = load_model(\"./models/BiLSTM_3.hdf5\")\n",
    "prediction = model.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "\n",
    "\n",
    "# prediction_list = []\n",
    "# for path in model_name:\n",
    "#     model = load_model(path)\n",
    "#     prediction = model.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "#     prediction_list.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_total = prediction1 + prediction2 + prediction3 + prediction4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.argmax(prediction,axis=1)\n",
    "print(\"accuracy:\",accuracy_score(pred_y.tolist(), y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LabelsMapping_inv =  {v: k for k, v in LabelsMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = [LabelsMapping_inv[v] for v in pred_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = list(range(8001,8001+len(pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"proposed_answers.txt\", \"w\") as f:\n",
    "    for i in range(len(test_id)):\n",
    "        f.write(str(test_id[i])+\"\\t\"+pred_y[i])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Micro-averaged result (excluding Other):\n",
    "P = 1956/2367 =  82.64%     R = 1956/2263 =  86.43%     F1 =  84.49%\n",
    "\n",
    "MACRO-averaged result (excluding Other):\n",
    "P =  82.12%     R =  85.90%     F1 =  83.89%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
