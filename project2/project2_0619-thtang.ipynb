{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle as pk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding,Input,BatchNormalization,Dense,Bidirectional,LSTM,Dropout\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.merge import add, dot, concatenate, multiply, average\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LabelsMapping = {'Other': 0,\n",
    "                 'Message-Topic(e1,e2)': 1, 'Message-Topic(e2,e1)': 2,\n",
    "                 'Product-Producer(e1,e2)': 3, 'Product-Producer(e2,e1)': 4,\n",
    "                 'Instrument-Agency(e1,e2)': 5, 'Instrument-Agency(e2,e1)': 6,\n",
    "                 'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
    "                 'Cause-Effect(e1,e2)': 9, 'Cause-Effect(e2,e1)': 10,\n",
    "                 'Component-Whole(e1,e2)': 11, 'Component-Whole(e2,e1)': 12,\n",
    "                 'Entity-Origin(e1,e2)': 13, 'Entity-Origin(e2,e1)': 14,\n",
    "                 'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16,\n",
    "                 'Content-Container(e1,e2)': 17, 'Content-Container(e2,e1)': 18}\n",
    "\n",
    "def _shuffle(X, feature1, feature2, y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], feature1[randomize], feature2[randomize], y[randomize])\n",
    "\n",
    "def load_data(FILE_Path):\n",
    "    otuput_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(FILE_Path, \"r\") as f:\n",
    "        train_tmp = f.read().split(\"\\n\\n\")[:-1]\n",
    "        for sample in train_tmp:\n",
    "            sentence = re.split('\\t|\\n', sample)[1].replace('\\\"',\"\").replace(\".\",\"\").replace(\",\",\"\")\\\n",
    "            .replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"\\'s\",\"\")\\\n",
    "            .replace(\"\\'ve\",\"\").replace(\"\\'t\",\"\").replace(\"\\'re\",\"\").replace(\"\\'d\",\"\").replace(\"\\'ll\",\"\")\n",
    "            answer = re.split('\\t|\\n', sample)[2]\n",
    "            soup = BeautifulSoup(sentence,\"lxml\")\n",
    "\n",
    "            e1 = str(soup.find('e1'))[4:-5]\n",
    "            e2 = str(soup.find('e2'))[4:-5]\n",
    "            word_list = sentence.split(\" \")\n",
    "            for word in word_list:\n",
    "                if \"</e1>\" in word:\n",
    "                    a = word\n",
    "                if \"</e2>\" in word:\n",
    "                    b = word\n",
    "            p1 = word_list.index(a)\n",
    "            p2 = word_list.index(b)\n",
    "            sentence = sentence.replace(\"<e1>\",\"\").replace(\"</e1>\",\"\").replace(\"<e2>\",\"\").\\\n",
    "            replace(\"</e2>\",\"\")\n",
    "#             sentence = [lemmatizer.lemmatize(word, pos='v') for word in sentence.split(\" \")]\n",
    "#             sentence = \" \".join(sentence)\n",
    "            otuput_sent.append((sentence,e1,e2,p1,p2,answer))\n",
    "    return otuput_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances: 8000\n",
      "number of testing instances: 2717\n"
     ]
    }
   ],
   "source": [
    "train_instance = load_data('./data/TRAIN_FILE.txt')\n",
    "test_instance = load_data('./data/TEST_FILE_FULL.txt')\n",
    "# sentence_list = train_sentence + test_sentence\n",
    "print(\"number of training instances:\", len(train_instance))\n",
    "print(\"number of testing instances:\", len(test_instance))\n",
    "# tuple format: (text, e1, e2, pos1, pos2, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "num_classes = 19\n",
    "train_text = []\n",
    "for instance in train_instance:\n",
    "    p1 = instance[3]\n",
    "    p2 = instance[4]\n",
    "    prune_text = \" \".join(instance[0].split(\" \")[p1:p2+1])\n",
    "    train_text.append(prune_text)\n",
    "\n",
    "train_label = [LabelsMapping[ele[5]] for ele in train_instance]\n",
    "train_label = dense_to_one_hot(np.array(train_label), 19)\n",
    "\n",
    "test_text = []\n",
    "for instance in test_instance:\n",
    "    p1 = instance[3]\n",
    "    p2 = instance[4]\n",
    "    prune_text = \" \".join(instance[0].split(\" \")[p1:p2+1])\n",
    "    test_text.append(prune_text)\n",
    "    \n",
    "test_label = [LabelsMapping[ele[5]] for ele in test_instance]\n",
    "test_label = dense_to_one_hot(np.array(test_label), 19)\n",
    "\n",
    "total_text = train_text + test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = np.load(\"./features/train_mut_ancestors_list.npy\")\n",
    "test_mut_ancestors_list = np.load(\"./features/test_mut_ancestors_list.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dep_list = np.load(\"./features/train_dep_list.npy\")\n",
    "test_dep_list = np.load(\"./features/test_dep_list.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11518 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=25000,lower=True,split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(total_text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 33\n"
     ]
    }
   ],
   "source": [
    "train_sentence_seq = tokenizer.texts_to_sequences(train_text)\n",
    "test_sentence_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "max_length = np.max([len(i) for i in train_sentence_seq+test_sentence_seq])\n",
    "print(\"max length:\", max_length)\n",
    "\n",
    "x_train_seq = sequence.pad_sequences(train_sentence_seq, maxlen=max_length)\n",
    "x_test_seq = sequence.pad_sequences(test_sentence_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# download pre-trained word vector from \"https://nlp.stanford.edu/projects/glove/\"\n",
    "tmp_file = get_tmpfile(\"/home/thtang/LifeLog/data/glove_pretrained/gensim_crawl_300d.txt\")\n",
    "\n",
    "w2vModel = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV: 218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "embedding_size = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "oov = 0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2vModel.wv.vocab:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = w2vModel[word]\n",
    "    else:\n",
    "        oov+=1\n",
    "print(\"OOV:\",oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Embedding, Input,InputLayer,BatchNormalization, Dense, Bidirectional,LSTM,Dropout,GRU,Activation\n",
    "from keras import backend as K\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "def train_BiLSTM(x_train, ancestor_train, dep_train, y_train, \n",
    "                 x_val, ancestor_val, dep_val, y_val,\n",
    "                 embedding_matrix, max_length, max_features):\n",
    "    embedding_size = 300\n",
    "    batch_size = 64\n",
    "    epochs = 50\n",
    "    embedding_layer = Embedding(max_features,output_dim= embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm0 = Bidirectional(LSTM(256,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(embedded_sequences)\n",
    "    lstm1 = Bidirectional(LSTM(128,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(lstm0)\n",
    "    lstm2 = Bidirectional(LSTM(64,activation=\"tanh\",dropout=0.2,return_sequences = False,\n",
    "                kernel_initializer='he_uniform'))(lstm1)\n",
    "    bn1 = BatchNormalization()(lstm2)\n",
    "    \n",
    "    # other feature inputs \n",
    "    ancestor_input = Input(shape=(2,))\n",
    "    ancestor_feature = Dense(64, activation=swish)(ancestor_input)\n",
    "    \n",
    "    \n",
    "    dep_input = Input(shape=(34,))\n",
    "    dep_feature = Dense(128, activation=swish)(dep_input)\n",
    "    \n",
    "    combine_feature = concatenate([bn1, ancestor_feature, dep_feature])\n",
    "    dense1 = Dense(64, activation=swish)(combine_feature)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(32, activation=swish)(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense2)\n",
    "    preds = Dense(19, activation='softmax')(dropout2)\n",
    "    model = Model([sequence_input, ancestor_input, dep_input], preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    filepath = \"models/BiLSTM_3.hdf5\" \n",
    "    checkpoint = ModelCheckpoint(filepath,monitor='val_acc',save_best_only=True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    history = model.fit([x_train, ancestor_train, dep_train], y_train, \n",
    "                        validation_data=([x_val, ancestor_val, dep_val], y_val), \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 2.3867 - acc: 0.2791 - val_loss: 1.4632 - val_acc: 0.5812\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.7870 - acc: 0.4626 - val_loss: 1.2653 - val_acc: 0.6360\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 1.5295 - acc: 0.5346 - val_loss: 1.0808 - val_acc: 0.6798\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 1.3510 - acc: 0.5944 - val_loss: 1.0334 - val_acc: 0.7085\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 1.2196 - acc: 0.6399 - val_loss: 0.9697 - val_acc: 0.7240\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 1.1028 - acc: 0.6775 - val_loss: 0.9433 - val_acc: 0.7302\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.9871 - acc: 0.7099 - val_loss: 0.9066 - val_acc: 0.7453\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.9199 - acc: 0.7368 - val_loss: 0.9014 - val_acc: 0.7490\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.8366 - acc: 0.7622 - val_loss: 0.8777 - val_acc: 0.7611\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7576 - acc: 0.7879 - val_loss: 0.9721 - val_acc: 0.7442\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.7350 - acc: 0.7959 - val_loss: 0.9289 - val_acc: 0.7615\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6379 - acc: 0.8250 - val_loss: 0.9442 - val_acc: 0.7608\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5991 - acc: 0.8304 - val_loss: 0.9567 - val_acc: 0.7648\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.5642 - acc: 0.8423 - val_loss: 1.0041 - val_acc: 0.7652\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.5185 - acc: 0.8518 - val_loss: 1.1347 - val_acc: 0.7549\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4898 - acc: 0.8646 - val_loss: 1.0484 - val_acc: 0.7751\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.4551 - acc: 0.8787 - val_loss: 1.0030 - val_acc: 0.7788\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.3948 - acc: 0.8967 - val_loss: 1.0977 - val_acc: 0.7828\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.3617 - acc: 0.9016 - val_loss: 1.2206 - val_acc: 0.7777\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.3360 - acc: 0.9128 - val_loss: 1.2635 - val_acc: 0.7501\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.3131 - acc: 0.9209 - val_loss: 1.2898 - val_acc: 0.7678\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3078 - acc: 0.9225 - val_loss: 1.3095 - val_acc: 0.7792\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2919 - acc: 0.9264 - val_loss: 1.3365 - val_acc: 0.7641\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.2582 - acc: 0.9341 - val_loss: 1.3232 - val_acc: 0.7799\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.2523 - acc: 0.9391 - val_loss: 1.4304 - val_acc: 0.7810\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.2464 - acc: 0.9385 - val_loss: 1.4334 - val_acc: 0.7725\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.2472 - acc: 0.9426 - val_loss: 1.4592 - val_acc: 0.7784\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2276 - acc: 0.9431 - val_loss: 1.4538 - val_acc: 0.7692\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2281 - acc: 0.9472 - val_loss: 1.5327 - val_acc: 0.7637\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.2269 - acc: 0.9475 - val_loss: 1.4251 - val_acc: 0.7762\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2006 - acc: 0.9520 - val_loss: 1.4983 - val_acc: 0.7626\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1785 - acc: 0.9589 - val_loss: 1.6156 - val_acc: 0.7744\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.1708 - acc: 0.9594 - val_loss: 1.5907 - val_acc: 0.7714\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1671 - acc: 0.9567 - val_loss: 1.5531 - val_acc: 0.7637\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1452 - acc: 0.9660 - val_loss: 1.7465 - val_acc: 0.7766\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.1422 - acc: 0.9660 - val_loss: 1.7140 - val_acc: 0.7725\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.1544 - acc: 0.9609 - val_loss: 1.5681 - val_acc: 0.7729\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1401 - acc: 0.9646 - val_loss: 1.6539 - val_acc: 0.7795\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1550 - acc: 0.9647 - val_loss: 1.7268 - val_acc: 0.7729\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1377 - acc: 0.9686 - val_loss: 1.8701 - val_acc: 0.7578\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.1715 - acc: 0.9613 - val_loss: 1.6898 - val_acc: 0.7685\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1272 - acc: 0.9715 - val_loss: 1.7570 - val_acc: 0.7784\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1480 - acc: 0.9656 - val_loss: 1.6475 - val_acc: 0.7711\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1233 - acc: 0.9712 - val_loss: 1.6515 - val_acc: 0.7806\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1112 - acc: 0.9759 - val_loss: 1.7663 - val_acc: 0.7692\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1251 - acc: 0.9715 - val_loss: 1.9004 - val_acc: 0.7656\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1356 - acc: 0.9690 - val_loss: 1.7798 - val_acc: 0.7777\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1382 - acc: 0.9696 - val_loss: 1.6978 - val_acc: 0.7689\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1163 - acc: 0.9762 - val_loss: 1.7429 - val_acc: 0.7740\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0950 - acc: 0.9770 - val_loss: 1.8239 - val_acc: 0.7725\n"
     ]
    }
   ],
   "source": [
    "train_X, train_ancestor, train_dep, train_y = _shuffle(x_train_seq, \n",
    "                                                       train_mut_ancestors_list,\n",
    "                                                       train_dep_list,\n",
    "                                                       train_label)\n",
    "model = train_BiLSTM(train_X, train_ancestor, train_dep, train_y, \n",
    "                     x_test_seq, test_mut_ancestors_list, test_dep_list, test_label,\n",
    "                     embedding_matrix,\n",
    "                     max_length,\n",
    "                     len(word_index)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [np.where(r==1)[0][0] for r in test_label ]\n",
    "\n",
    "model_name = ['models/BiLSTM_0817.hdf5','models/BiLSTM_08175.hdf5',\n",
    "              'models/BiLSTM_0820.hdf5','models/BiLSTM_0821.hdf5']\n",
    "\n",
    "model1 = load_model(model_name[0])\n",
    "prediction1 = model1.predict([x_test_seq], batch_size=1000)\n",
    "model2 = load_model(model_name[1])\n",
    "prediction2 = model2.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "model3 = load_model(model_name[2])\n",
    "prediction3 = model3.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "model4 = load_model(model_name[3])\n",
    "prediction4 = model4.predict([x_test_seq,test_mut_ancestors_list], batch_size=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prediction_list = []\n",
    "# for path in model_name:\n",
    "#     model = load_model(path)\n",
    "#     prediction = model.predict([x_test_seq,test_mut_ancestors_list,test_dep_list], batch_size=1000)\n",
    "#     prediction_list.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_total = prediction1 + prediction2 + prediction3 + prediction4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8045638571954361\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.argmax(prediction_total,axis=1)\n",
    "print(\"accuracy:\",accuracy_score(pred_y.tolist(), y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LabelsMapping_inv =  {v: k for k, v in LabelsMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = [LabelsMapping_inv[v] for v in pred_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = list(range(8001,8001+len(pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"proposed_answers.txt\", \"w\") as f:\n",
    "    for i in range(len(test_id)):\n",
    "        f.write(str(test_id[i])+\"\\t\"+pred_y[i])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Micro-averaged result (excluding Other):\n",
    "P = 1956/2367 =  82.64%     R = 1956/2263 =  86.43%     F1 =  84.49%\n",
    "\n",
    "MACRO-averaged result (excluding Other):\n",
    "P =  82.12%     R =  85.90%     F1 =  83.89%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
