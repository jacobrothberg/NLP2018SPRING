{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pickle as pk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print (lemmatizer.lemmatize('working', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(FILE_Path):\n",
    "    otuput_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(FILE_Path, \"r\") as f:\n",
    "        train_tmp = f.read().split(\"\\n\\n\")[:-1]\n",
    "        for sample in train_tmp:\n",
    "            sentence = re.split('\\t|\\n', sample)[1].replace('\\\"',\"\").replace(\".\",\"\").replace(\",\",\"\")\\\n",
    "            .replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"\\'s\",\"\")\\\n",
    "            .replace(\"\\'ve\",\"\").replace(\"\\'t\",\"\").replace(\"\\'re\",\"\").replace(\"\\'d\",\"\")\\\n",
    "            .replace(\"\\'ll\",\"\").replace(\"'\",\"\").replace(\";\",\"\").replace(\":\",\"\")\n",
    "            answer = re.split('\\t|\\n', sample)[2]\n",
    "            soup = BeautifulSoup(sentence,\"lxml\")\n",
    "\n",
    "            e1 = str(soup.find('e1'))[4:-5]\n",
    "            e2 = str(soup.find('e2'))[4:-5]\n",
    "            word_list = sentence.split(\" \")\n",
    "            e1_check = 0\n",
    "            e2_check = 0\n",
    "            for word in word_list:\n",
    "                if word.endswith(\"</e1>\"):\n",
    "                    e1_check = 1\n",
    "                if word.endswith(\"</e2>\"):\n",
    "                    e2_check = 1\n",
    "            if e1_check + e2_check != 2:\n",
    "                print(sentence)\n",
    "            for word in word_list:\n",
    "                if \"</e1>\" in word:\n",
    "                    a = word\n",
    "                if \"</e2>\" in word:\n",
    "                    b = word\n",
    "            p1 = word_list.index(a)\n",
    "            p2 = word_list.index(b)\n",
    "            sentence = sentence.replace(\"<e1>\",\"\").replace(\"</e1>\",\"\").replace(\"<e2>\",\"\").\\\n",
    "            replace(\"</e2>\",\"\")\n",
    "            sentence = [lemmatizer.lemmatize(word, pos='v') for word in sentence.split(\" \")]\n",
    "            sentence = \" \".join(sentence)\n",
    "            otuput_sent.append((sentence,e1,e2,p1,p2,answer))\n",
    "    return otuput_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n"
     ]
    }
   ],
   "source": [
    "train_instance = load_data(\"./data/TRAIN_FILE.txt\")\n",
    "print(\"--------\")\n",
    "test_instance = load_data(\"./data/TEST_FILE_FULL.txt\")\n",
    "total = train_instance + test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When thecoil of the relay be at rest not energize the common terminal 30 and the normally close terminal 87a have continuity\n"
     ]
    }
   ],
   "source": [
    "print(train_text[4783])\n",
    "#train_position[4783]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_position = [(ele[3],ele[4]) for ele in train_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check p1 p2 order\n",
    "for a, b in train_position:\n",
    "    if a > b: \n",
    "        print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [ele[0] for ele in train_instance]\n",
    "test_test = [ele[0] for ele in test_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great wing of fluorite dragons the greatest concentration of the rather solitary dragons ever know join the desperate battle with the blue and silver against Chaos\n",
      "wing NN nsubj xxxx False 0 [concentration, know] lefts ['A', 'great'] rights ['of'] 2 1\n",
      "dragons NNS pobj xxxx False 0 [of, wing, concentration, know] lefts ['fluorite'] rights [] 1 0\n"
     ]
    }
   ],
   "source": [
    "demo_id = 62\n",
    "demo_sentence = train_text[demo_id]\n",
    "print(demo_sentence)\n",
    "\n",
    "doc = nlp(demo_sentence)\n",
    "\n",
    "for pos, token in enumerate(doc):\n",
    "    if pos in train_position[demo_id]:\n",
    "        ancestors = [i for i in token.ancestors]\n",
    "        print(token.text, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_stop, token.ent_type,ancestors,\n",
    "              \"lefts\",[word.text for word in token.lefts],\n",
    "              \"rights\",[word.text for word in token.rights],\n",
    "              token.n_lefts,  # 2\n",
    "            token.n_rights  # 1\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = []\n",
    "train_dep_list = []\n",
    "for ele in train_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    train_mut_ancestors_list.append(mut_ancestors)\n",
    "    train_dep_list.append(both_dep)\n",
    "    \n",
    "test_mut_ancestors_list = []\n",
    "test_dep_list = []\n",
    "for ele in test_instance:\n",
    "    sentence = ele[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity = ele[1:3]\n",
    "    both_ancestors = []\n",
    "    both_dep = \"\"\n",
    "    for pos, token in enumerate(doc):\n",
    "        if pos in ele[3:5]:\n",
    "            ancestors = [i.text for i in token.ancestors]\n",
    "            both_ancestors.append(ancestors)\n",
    "            both_dep+=token.dep_\n",
    "    a = 0\n",
    "    b = 0\n",
    "    if entity[0] in both_ancestors[1]:\n",
    "        a = 1\n",
    "    elif entity[1] in both_ancestors[0]:\n",
    "        b = 1\n",
    "    mut_ancestors = [a,b]\n",
    "    test_mut_ancestors_list.append(mut_ancestors)\n",
    "    test_dep_list.append(both_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_mut_ancestors_list.npy\", train_mut_ancestors_list)\n",
    "np.save(\"./features/test_mut_ancestors_list.npy\", test_mut_ancestors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(train_dep_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sorted = a[np.argsort(b)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nsubjpobj', 'dobjpobj', 'pobjpobj', 'nsubjdobj', 'nsubjpasspobj',\n",
       "       'compoundpobj', 'nsubjcompound', 'attrpobj', 'compoundnsubj',\n",
       "       'conjpobj', 'compounddobj', 'pobjdobj', 'nsubjnsubj', 'nsubjpunct',\n",
       "       'pobjcompound', 'compoundcompound', 'amodpobj', 'dobjdobj',\n",
       "       'dobjcompound', 'nsubjconj', 'ROOTpobj', 'nsubjpasscompound',\n",
       "       'nsubjpassdobj', 'nsubjamod', 'pobjconj', 'attrdobj', 'punctprep',\n",
       "       'appospobj', 'pobjnsubj', 'compoundattr', 'pobjattr', 'nsubjnmod',\n",
       "       'amoddobj'], dtype='<U18')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_dep = a_sorted[:33]\n",
    "major_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1793,\n",
       " 940,\n",
       " 846,\n",
       " 842,\n",
       " 693,\n",
       " 326,\n",
       " 171,\n",
       " 153,\n",
       " 133,\n",
       " 111,\n",
       " 110,\n",
       " 87,\n",
       " 83,\n",
       " 66,\n",
       " 65,\n",
       " 55,\n",
       " 50,\n",
       " 50,\n",
       " 48,\n",
       " 47,\n",
       " 47,\n",
       " 41,\n",
       " 38,\n",
       " 37,\n",
       " 27,\n",
       " 27,\n",
       " 26,\n",
       " 26,\n",
       " 25,\n",
       " 24,\n",
       " 20,\n",
       " 20,\n",
       " 20]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b,reverse=True)[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dep_list_filter = []\n",
    "for dep in train_dep_list:\n",
    "    if dep in major_dep:\n",
    "        train_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        train_dep_list_filter.append(\"other\")\n",
    "\n",
    "test_dep_list_filter = []\n",
    "for dep in test_dep_list:\n",
    "    if dep in major_dep:\n",
    "        test_dep_list_filter.append(dep)\n",
    "    else:\n",
    "        test_dep_list_filter.append(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ROOTpobj', 'amoddobj', 'amodpobj', 'appospobj', 'attrdobj',\n",
       "        'attrpobj', 'compoundattr', 'compoundcompound', 'compounddobj',\n",
       "        'compoundnsubj', 'compoundpobj', 'conjpobj', 'dobjcompound',\n",
       "        'dobjdobj', 'dobjpobj', 'nsubjamod', 'nsubjcompound', 'nsubjconj',\n",
       "        'nsubjdobj', 'nsubjnmod', 'nsubjnsubj', 'nsubjpasscompound',\n",
       "        'nsubjpassdobj', 'nsubjpasspobj', 'nsubjpobj', 'nsubjpunct',\n",
       "        'other', 'pobjattr', 'pobjcompound', 'pobjconj', 'pobjdobj',\n",
       "        'pobjnsubj', 'pobjpobj', 'punctprep'], dtype='<U17'),\n",
       " array([ 22,   7,  13,   3,  11,  61,   8,  16,  38,  24, 102,  23,  17,\n",
       "         22, 318,  12,  49,  22, 247,   8,  25,  22,  15, 239, 652,  26,\n",
       "        320,  10,  28,   9,  38,   9, 290,  11], dtype=int64))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_dep_list_filter, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform string to onehot\n",
    "train_dep_list_filter = lb.fit_transform(train_dep_list_filter)\n",
    "test_dep_list_filter = lb.transform(test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_dep_list.npy\", train_dep_list_filter)\n",
    "np.save(\"./features/test_dep_list.npy\", test_dep_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dep_list_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dep_list_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## WordNet hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_list = []\n",
    "for idx in range(len(total)):\n",
    "    entity_1 = total[idx][1]\n",
    "    entity_2 = total[idx][2]\n",
    "    \n",
    "    synsets = wn.synsets(entity_1, pos=wn.NOUN)\n",
    "    hyper_list_1 = []\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            hyper_list_1 = hyper_list_1 + [x.lower() for x in hyper.lemma_names() if '_' not in x]\n",
    "    \n",
    "    synsets = wn.synsets(entity_2, pos=wn.NOUN)\n",
    "    hyper_list_2 = []\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            hyper_list_2 = hyper_list_2 + [x.lower() for x in hyper.lemma_names() if '_' not in x]\n",
    "    hyper = ' '.join(list(set(hyper_list_1).union(set(hyper_list_2)))) \n",
    "    hyper_list.append(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hyper_list = hyper_list[:8000]\n",
    "test_hyper_list = hyper_list[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_hyper_list.npy\", train_hyper_list)\n",
    "np.save(\"./features/test_hyper_list.npy\", test_hyper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rootage descendant descendent somebody individual offspring mortal issue someone root soul trough juvenile progeny beginning birth person source origin'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hyper_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PropBank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict/propBank_dict.pk', 'rb') as f:\n",
    "    propBank_dict = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "propBank_verb_entity_dict = {}\n",
    "hyper_verb_list = []\n",
    "for idx in range(len(total)):\n",
    "    prop_rule = []\n",
    "    temp_dict = {}\n",
    "    for prop_rule in propBank_dict[idx]['propBank']:\n",
    "        verb = prop_rule[0].split('.')[0]\n",
    "        verb = lmtzr.lemmatize(verb,'v')\n",
    "        entity = prop_rule[4]\n",
    "        if len(entity.split(' ')) > 1:\n",
    "            entity = entity.split(' ')[-1]\n",
    "        #print(entity)\n",
    "        if verb in temp_dict:\n",
    "            temp_dict[verb].append(entity)\n",
    "        else:\n",
    "            temp_dict[verb] = [entity]\n",
    "    \n",
    "    temp_hyper_list = []\n",
    "    for k, v in temp_dict.items():\n",
    "        if total[idx][1] in v and total[idx][2] in v:\n",
    "            synsets = wn.synsets(k, pos=wn.VERB)\n",
    "            temp_hyper_list.append(k)\n",
    "\n",
    "            for syn in synsets:\n",
    "                if len(syn.hypernyms()) != 0:\n",
    "                    temp_hyper_list = temp_hyper_list + [x.lower() for x in syn.hypernyms()[0].lemma_names() if '_' not in x]\n",
    "            \n",
    "    hyper = ' '.join(list(set(temp_hyper_list)))\n",
    "    hyper_verb_list.append(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'relate fix fasten stick adhere indispose restrain bond bind attach secure cover hold confine',\n",
       " 'ingest use work have take act consume move exploit',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_verb_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hyper_verb_list = hyper_verb_list[:8000]\n",
    "test_hyper_verb_list = hyper_verb_list[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./features/train_hyper_verb_list.npy\", train_hyper_verb_list)\n",
    "np.save(\"./features/test_hyper_verb_list.npy\", test_hyper_verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
